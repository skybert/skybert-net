<!--#include virtual="/ssi/header.shtml" -->
    <h1>Some of My Favourite wget Tricks</h1>
    <div id="main">
      <h2>Simple Load Testing with wget</h2>
      <p>
        To do a <em>simple</em> load test, <code>wget</code> can be used
        like this to download full pages, recursively (depth five) and
        repeat everything 30 times. This simple load test is a useful
        starting point to iron out the most obvious performance
        issues. 
      </p>
      <p>
        Once this doesn't pose any problem, time is ripe to start using
        more serious stress testing tools
        like <a href="http://www.hpl.hp.com/research/linux/httperf/">
          httperf
        </a>
        and <a href="http://www.joedog.org/index/siege-home">
          siege
        </a>
      </p>
<pre>
$ for i in $(seq 30); do
    wget -o /dev/null \
         -p \
         http://mysite.com \
  done
</pre>
      <h2>Populating Your Caches with wget</h2>
      <p>
        This is how I populate my caches, both application caches,
        distributed memory caches and cache server caches are simply
        populated with <code>wget</code>. This will traverse the site
        recursively with the default depth <code>5</code> and delete the
        files after downloading them.
      </p>
<pre>
$ wget -o /dev/null \
       -r \
       --delete-after \
       http://mysite.com
</pre>
      <p>
        Please note that if you give <code>/dev/null</code> to
        the <code>-O</code> parameter (big 'o'), <code>--delete-after</code>
        actually remove the <code>/dev/null</code> file - if your OS and
        user rights allow it! The command above is safe, though. Just
        wanted to warn you as I've done this mistake before ;-)
      </p>
      <h2>Timing Your Site Delivery Time</h2>
      <p>
        This is how I time the delivery time of the web sites. It's
        important to time it many times, as you may hit the server(s)
        at the bad time when they're doing garbage collections,
        invalidate their caches etc. Thus, I always to 10 fetches to
        determine the delivery speed:
      </p>
<pre>
$ for i in $(seq 30); do 
    time wget -p \
      --delete-after \
      -o /dev/null \
      http://mysite.com/
  done
</pre>
    </div>
<!--#include virtual="/ssi/footer.shtml" -->
